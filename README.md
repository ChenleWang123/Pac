# PACMAN-RL Team Project TUD

A Pacman game implementation that compares **Reinforcement Learning (DQN)** with **Behavioural Cloning (Imitation Learning)** approaches for game AI.

This repository is **an extended version based on the original project by [Jmansford](https://github.com/Jmansford/Pacman-RL-VS-BC)**, with additional features and innovations developed as part of a **team project** by  
**Yi Yang, Kaiyue Yang, and Chenle Wang** at **Technische UniversitÃ¤t Dresden**.
- <img src="images/gameplay.png" alt="Game Screenshot" style="width:50%;">
---

## ðŸ§  Project Overview

This project implements a Pacman game environment where the player (Pacman) can be controlled by either:
1. A **Deep Q-Network (DQN)** trained through reinforcement learning
2. A **Behavioural Cloning** model trained through imitation learning

The goal is to compare the performance and learning characteristics of these two different approaches to game AI.

In our **extended version**, we introduced **three new gameplay abilities** and expanded the **state observation dimension** from **42 â†’ 43**, enabling the agent to learn **skill-aware decision making**.

---

## ðŸš€ New Features (Developed by Yi Yang, Kaiyue Yang, and Chenle Wang)

### ðŸŸ¢ Added Power-Ups
We introduced three new gameplay abilities that can be triggered and tracked by the AI:
1. **Boost** â€“ temporarily increases Pacmanâ€™s speed  
- <img src="images/boost.png" alt="Game Screenshot" style="width:50%;">
2. **Invisibility** â€“ makes Pacman invisible to ghosts for a short duration
- <img src="images/invisibility.png" alt="Game Screenshot" style="width:50%;">
3. **Freeze** â€“ freezes all ghosts temporarily  
- <img src="images/freeze.png" alt="Game Screenshot" style="width:50%;">

### ðŸ§© State Space Expansion (42D â†’ 43D)

To allow the RL agent to perceive its current skill status, we added a new **Pacman status feature vector**:

```python
pacman_status_features = np.array([
    is_boosted, boost_timer_normalized,
    is_invisible, invisible_timer_normalized,
    is_freezing, freeze_timer_normalized
], dtype=np.float32)
```

These new 6 status features are concatenated with the original 42-dimensional observation, forming a **43-dimensional** total state vector:

```python
full_observation = np.concatenate([
    pacman_status_features,
    base_observation,
    enhanced_features
])
```

| Version     | State Dimensions | Skill State Features | Skill State Awareness | Behavioral Intelligence Improvement |
|--------------|------------------|----------------------|------------------------|--------------------------------------|
| Old 42-dim   | 42               | None                 | No                     | Cannot distinguish normal/boost/invisible states |
| New 43-dim   | 43               | 6 skill state features | Yes                   | Can intelligently use all skills |

---

## ðŸ“ˆ Evaluation

During DQN training, the **43D model** shows higher average reward and faster convergence compared to the **42D baseline**.

<div align="center">
  <img src="images/evaluation1.png" alt="Evaluation Results" style="width:80%;">
  <img src="images/evaluation.png" alt="Evaluation Results" style="width:80%;">
</div>

*Comparison of average scores over 500k timesteps (Dark Blue: 43D, Light Blue: 42D)*

The extended state dimension enables the RL agent to:
- Recognize and utilize power-up states effectively  
- Show adaptive behavior (e.g., using invisibility when surrounded by ghosts)  
- Achieve higher cumulative rewards  

---

## ðŸ•¹ï¸ Classic Features (from Original Project)

- Classic Pacman gameplay with ghosts and pellets  
- Two maze configurations: **Simple** and **Complex**  
- DQN-based Reinforcement Learning and Imitation Learning agents  
- Expert demonstration recording and playback  
- Enhanced feature extraction for better state representation  
- TensorBoard integration for training visualization  

---

## ðŸŽ® Controls

### Game Modes
- `1`: HUMAN mode (manual control)  
- `2`: A_STAR mode (automatic pathfinding)  
- `3`: DQN mode (reinforcement learning)  
- `4`: IMITATION mode (behavioural cloning)  

### Human Mode Controls
- `â†‘ â†“ â† â†’`: Move directions  

### Game Management
- `M`: Switch between Simple and Complex maze  
- `SPACE`: Restart game  

### Demonstration Recording
- `R`: Start recording (in HUMAN mode)  
- `T`: Stop recording  
- `C`: Cancel current recording  
- `A`: Analyse recorded demonstrations  

---

## ðŸ§© Project Structure

```
PACMAN-RL-VS-BC/
â”œâ”€â”€ dqn/              # Deep Q-Network implementation
â”œâ”€â”€ imitation/        # Behavioural Cloning implementation
â”œâ”€â”€ entities/         # Game entities (Pacman, Ghosts)
â”œâ”€â”€ rendering/        # Game rendering utilities
â”œâ”€â”€ utils/            # Helper functions and utilities
â”œâ”€â”€ experiments/      # Experiment configurations and results
â”œâ”€â”€ outputs/          # Model outputs and saved data
â”œâ”€â”€ images/           # Screenshots and result plots
â”œâ”€â”€ main.py           # Main game loop and entry point
â””â”€â”€ constants.py      # Game constants and configurations
```

---

## âš™ï¸ Requirements

- Python 3.x  
- Pygame  
- PyTorch  
- NumPy  

Install dependencies:
```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

### Run the Game
```bash
python main.py
```

### Training Modes
- **DQN Training** â†’ Set `TRAINING_MODE = True`  
- **Imitation Learning** â†’ Set `IMITATION_MODE = True` and `TRAIN_IMITATION_MODEL = True`  
- **Record Expert Demonstrations** â†’ Set `RECORD_DEMONSTRATIONS = True`  

### TensorBoard Visualization
```bash
tensorboard --logdir=./outputs/logs/tensorboard
```
Then open [http://localhost:6006](http://localhost:6006)

---

## ðŸ§‘â€ðŸ’» Credits

- **Original Author:** [Jmansford](https://github.com/Jmansford/Pacman-RL-VS-BC)  
- **Extended Version Developers:** Yi Yang, Kaiyue Yang, Chenle Wang  
- **Institution:** Technische UniversitÃ¤t Dresden  
- **Course Project:** Reinforcement Learning & Behavioural Cloning Comparison for Pacman AI  

---

## ðŸ“š Citation

If you use or reference this project, please credit both the original author and our extended version contributors:

```
Original work by Jmansford.
Extended version with power-up features, skill-aware state expansion, and RL evaluation by
Yi Yang, Kaiyue Yang, and Chenle Wang, Technische UniversitÃ¤t Dresden (2025).
```

---

## ðŸ Results Summary

| Model       | State Dim | Avg. Score | Skill Usage                | Comment                             |
|--------------|------------|-------------|-----------------------------|-------------------------------------|
| DQN (42D)    | 42         | ~400        | None                        | Baseline RL agent                  |
| DQN (43D)    | 43         | ~600        | Boost + Invisible + Freeze  | Learns to trigger skills intelligently |

---
> ðŸ“¸ The figures shown above are from our presentation slides, illustrating innovation (dimension expansion) and evaluation results from our reinforcement learning experiments.
